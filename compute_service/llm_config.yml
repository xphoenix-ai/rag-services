hf:
  model_config:
    # model_path: microsoft/Phi-3-mini-4k-instruct
    # model_path: mistralai/Mistral-7B-Instruct-v0.2
    model_path: meta-llama/Meta-Llama-3-8B-Instruct
    torch_dtype: torch.float16
    low_cpu_mem_usage: True
    load_in_4bit: False
    load_in_8bit: False
    tokenize_with_chat_template: True
  generation_config:
    max_new_tokens: 200
    do_sample: True
    temperature: 0.1
    top_p: 0.95
    top_k: 20
    repetition_penalty: 1.0

llama_cpp:
  model_config:
    # model_path: bartowski/Meta-Llama-3-8B-Instruct-GGUF
    model_path: D:/Other/Singlish-RAG/rag_services/bot_backend/Phi-3-mini-4k-instruct-q4.gguf
    filename: "*Q4_K_M.gguf" 
    chat_format: llama-3
    tokenize_with_chat_template: True
  generation_config:
    max_tokens: 200
    temperature: 0.1
    top_p: 0.95
    top_k: 20
    
ollama:
  model_config:
    # model_path: llama3:instruct
#    model_path: phi3
    model_path: llama3.2:1b-instruct-q4_K_M
    keep_alive: 5m
    tokenize_with_chat_template: True
  generation_config:
    num_predict: 200
    temperature: 0.1
    top_p: 0.95
    top_k: 20
